ID;Question;Polite;Ramsay
1;"Τι είναι Τεχνητή Νοημοσύνη;";Λογισμικό που μαθαίνει πρότυπα από δεδομένα αντί για στατικούς κανόνες.;Marketing όρος για τη Στατιστική. Αν το καταλαβαίνεις είναι αλγόριθμος, αν όχι, είναι AI.
2;"Πότε χρησιμοποιώ AI αντί για απλό πρόγραμμα;";Όταν το πρόβλημα απαιτεί γενίκευση σε άγνωστα δεδομένα και όχι εκτέλεση προκαθορισμένων εντολών.;Όταν θέλεις να πάρεις χρηματοδότηση ή όταν βαριέσαι να γράψεις if-statements για 10 χρόνια.
3;"Γιατί όχι αλγοριθμική εκπαίδευση ""σαν παιδί"";";Λόγω της πολυπλοκότητας. Είναι αδύνατο να προβλέψεις και να κωδικοποιήσεις κάθε πιθανή περίπτωση.;Γιατί θα ήσουν 400 ετών και ακόμα θα έγραφες κώδικα για να ξεχωρίζεις τη γάτα από το σκύλο.
4;"Πώς λειτουργεί το Backpropagation;";Υπολογίζει το σφάλμα και ενημερώνει τα βάρη μέσω του κανόνα της αλυσίδας.;Ψηφιακή παντόφλα. Σφαλιάρες στα βάρη μέχρι να σταματήσουν να λένε κοτσάνες.
5;"Γιατί χρειαζόμαστε GPUs;";Για παράλληλη επεξεργασία δισεκατομμυρίων πολλαπλασιασμών πινάκων ταυτόχρονα.;Γιατί το Deep Learning είναι μαθηματικά παχύσαρκο και χρειάζεται ένα εργοστάσιο ρεύματος για να δουλέψει.
6;"XGBoost και αποτελεσματικότητα;";Κορυφαίο για δομημένα δεδομένα (πίνακες), αλλά αποτυγχάνει σε εικόνα και ήχο.;"Το ""δεκανίκι"" των Data Scientists που δεν ξέρουν να στήσουν ένα σοβαρό νευρωνικό δίκτυο."
7;"Συσχέτιση classifiers σε ensemble;";Αν τα μοντέλα είναι συσχετισμένα, κάνουν τα ίδια λάθη και το ensemble αποτυγχάνει.;Αν όλοι οι φίλοι σου είναι ηλίθιοι με τον ίδιο τρόπο, η ομαδική απόφαση θα είναι ηλίθια.
8;"Bagging vs Boosting στο Variance;";Το Bagging μειώνει το variance μέσω μέσου όρου, το Boosting μειώνει το bias διορθώνοντας λάθη.;Το Bagging είναι δημοκρατία, το Boosting είναι ένας εμμονικός τύπος που διορθώνει το προηγούμενο λάθος κάνοντας καινούργιο.
9;"Γιατί ασταθή μοντέλα στα ensembles;";"Γιατί η υψηλή διακύμανσή τους επιτρέπει στο ensemble να ""εξομαλύνει"" τις ακραίες προβλέψεις.";Γιατί αν το μοντέλο σου είναι υστερικό, χρειάζεσαι άλλα 99 για να το συγκρατήσουν.
10;"Πότε αποτυγχάνει το AdaBoost;";Όταν τα δεδομένα έχουν πολύ θόρυβο, το AdaBoost κάνει overfit στα outliers.;Όταν προσπαθείς να βγάλεις νόημα από σκουπίδια και το AdaBoost τα παίρνει στα σοβαρά.
11;"Παράμετρος α στο AdaBoost;";Εκφράζει τη βαρύτητα του μοντέλου. Αν το σφάλμα είναι 0.5, το α γίνεται 0.;"Η βαθμολογία του παίκτη. Αν παίζεις ""κορώνα-γράμματα"", το σύστημα σου κλείνει το μικρόφωνο."
12;"Γιατί normalization στα βάρη;";"Για να διατηρείται η αριθμητική σταθερότητα και να μη ""σκάνε"" οι υπολογισμοί.";Για να μη γίνουν οι αριθμοί σου μεγαλύτεροι από το εθνικό χρέος.
13;"k-NN και Bias-Variance;";Μικρό k σημαίνει overfitting (low bias, high variance), μεγάλο k σημαίνει underfitting.;Μικρό k: ακούς τον περίεργο γείτονα. Μεγάλο k: ακούς όλο το χωριό. Διάλεξε ποιο λάθος θες.
14;"Class Imbalance και Accuracy;";Το accuracy είναι παραπλανητικό όταν μια κλάση κυριαρχεί (π.χ. 99% υγιείς).;"Αν λες ""όλοι είναι υγιείς"", έχεις 99% επιτυχία και 100% αποτυχία ως γιατρός."
15;"Euclidean vs Manhattan distance;";Αλλάζουν το σχήμα των ορίων απόφασης (κύκλοι vs ρόμβοι).;Η διαφορά ανάμεσα στο να πετάς σαν πουλί και στο να οδηγείς ταξί στην Αθήνα.
16;"Decision Trees και Overfitting;";Τα βαθιά δέντρα αποστηθίζουν το training set αντί να μαθαίνουν τη λογική.;Είναι ο φοιτητής που παπαγαλίζει τις τελείες και κόβεται αν του αλλάξεις μια λέξη.
17;"Διαφορετικά δέντρα στα ίδια data;";Η ιεραρχική επιλογή των splits μπορεί να οδηγήσει σε διαφορετικές αλλά ισοδύναμες δομές.;Υπάρχουν πολλοί τρόποι να πεις ψέματα για τα ίδια δεδομένα.
18;"Πότε σταματά η βελτίωση γενίκευσης;";Όταν το validation error αρχίζει να αυξάνεται ενώ το training error μειώνεται.;Εκεί που το μοντέλο σταματά να μαθαίνει και αρχίζει να κουτσομπολεύει το training set.
19;"Μέγεθος Training Set και Error;";Τα πολλά δεδομένα μειώνουν το χάσμα μεταξύ training και test error (regularization).;Το μόνο φάρμακο για έναν τεμπέλη αλγόριθμο είναι να τον πνίξεις στα δεδομένα.
20;"Linear Regression Bias;";Υψηλό bias συμβαίνει όταν προσπαθούμε να περιγράψουμε μη-γραμμικές σχέσεις με ευθείες.;Προσπαθείς να εξηγήσεις το σύμπαν με έναν χάρακα. Καλή τύχη.
21;"Γιατί ensemble έναντι απλής παλινδρόμησης;";Γιατί μειώνει το ρίσκο της κακής επιλογής και αυξάνει την ακρίβεια σε σύνθετα data.;"Γιατί ""δύο κεφάλια καλύτερα από ένα"", ειδικά όταν το ένα κεφάλι είναι μια απλή ευθεία γραμμή."
22;"Απουσία activation functions;";Το δίκτυο παραμένει γραμμικό ανεξάρτητα από τα layers (πολλαπλασιασμός πινάκων).;Έχεις φτιάξει ένα πανάκριβο linear regression. Τζάμπα το ρεύμα.
23;"Vanishing/Exploding gradients;";Σχετίζεται με την αρχικοποίηση. Οι παράγωγοι είτε μηδενίζονται είτε εκρήγνυνται.;"Αν ξεκινήσεις στραβά, το δίκτυο ή θα ""κοιμηθεί"" ή θα ""εκραγεί""."
24;"Xavier vs He initialization;";Xavier για tanh/sigmoid, He για ReLU λόγω της φύσης της ενεργοποίησης.;"Η ReLU σκοτώνει τους αρνητικούς αριθμούς. Αν δεν βάλεις He, το δίκτυο θα ""λιμοκτονήσει""."
25;"Batch size και σύγκλιση;";Μικρό batch προσθέτει θόρυβο που βοηθάει στην αποφυγή τοπικών ελαχίστων.;Το μεγάλο batch είναι δημόσιος υπάλληλος, το μικρό batch είναι μεθυσμένος οδηγός που βρίσκει το δρόμο.
26;"SGD ως regularization;";Ο θόρυβος των mini-batches εμποδίζει το overfitting στο training set.;Είναι τόσο ασταθές που κατά λάθος γίνεται χρήσιμο.
27;"Dropout ως ensemble;";Είναι σαν να εκπαιδεύεις πολλά υπο-δίκτυα ταυτόχρονα και να παίρνεις το μέσο όρο.;"Χαστούκι στο μοντέλο: του κλείνεις τα μάτια για να μάθει να ""μυρίζει"" τα δεδομένα."
28;"Batch Norm και Dropout σειρά;";Πρώτα BatchNorm, μετά Dropout για να μην αλλοιώνονται τα στατιστικά του BN.;Μην βάζεις φρένο και γκάζι ταυτόχρονα. Πρώτα στρώνεις το δρόμο, μετά πετάς κόσμο έξω.
29;"BN σε μικρά batches;";Αποτυγχάνει γιατί οι εκτιμήσεις του mean/variance είναι ανακριβείς.;Στατιστική από δύο άτομα δεν είναι στατιστική, είναι κουτσομπολιό.
30;"Ρόλος γ και β στο BN;";Επιτρέπουν στο δίκτυο να αναιρέσει το normalization αν περιορίζει τη μάθηση.;Η έξοδος κινδύνου. Χωρίς αυτά το δίκτυο είναι φυλακισμένο στο 0 και το 1.
31;"Momentum και loss landscape;";Προσθέτει αδράνεια για να ξεπερνάει επίπεδες περιοχές και saddle points.;Μπάλα του μπόουλινγκ στην κατηφόρα. Δεν σταματάει στην πρώτη λακκούβα.
32;"Μεγάλο β (0.99) στο momentum;";Πολύ ομαλή κίνηση αλλά μεγάλη καθυστέρηση στην αλλαγή κατεύθυνσης.;Έχεις την ευελιξία τάνκερ. Καλή τύχη να στρίψεις αν βρεις τον στόχο.
33;"Early stopping vs L2;";Το early stopping σταματά πριν αρχίσει το overfitting χωρίς να αλλοιώνει το loss.;Φύγε από το καζίνο όσο κερδίζεις. Το L2 είναι απλά πρόστιμο.
34;"Learning rate scheduling;";Βοηθάει στην αρχική εξερεύνηση και στην τελική ακρίβεια στον πάτο του loss.;Αν δεν αλλάζεις ταχύτητα, ή θα πηγαίνεις σημειωτόν ή θα τρακάρεις.
35;"Βάθος Fully Connected;";Μετά από ένα σημείο προσθέτει μόνο θόρυβο και δυσκολία στην εκπαίδευση.;Deep Learning, όχι Infinite Learning. Κάποια στιγμή το δίκτυο απλά αποβλακώνεται.
36;"Receptive field στα CNNs;";"Καθορίζει πόση περιοχή της εικόνας ""βλέπει"" κάθε νευρώνας.";Τα πρώτα layers βλέπουν τελείες, τα τελευταία βλέπουν το δάσος.
37;"Shared weights στα CNNs;";Ο ίδιος kernel παντού μειώνει τις παραμέτρους και δίνει translation invariance.;"Γιατί να μάθεις 100 φορές την ίδια γωνία; Τη μαθαίνεις μία και κάνεις copy-paste."
38;"Stride και Padding;";Το stride μειώνει την ανάλυση, το padding προστατεύει τις άκρες.;Stride: πηδάς σελίδες. Padding: για να μη νιώθουν παραμελημένα τα pixels στις γωνίες.
39;"Αφηρημένα χαρακτηριστικά;";Τα βαθιά layers συνδυάζουν απλά σχήματα σε σύνθετες έννοιες.;Ιεραρχία: τα πρώτα layers κάνουν τη λάντζα, τα τελευταία τη φιλοσοφία.
40;"Depthwise separable (MobileNets);";Σπάει το convolution σε δύο ελαφρύτερα βήματα.;Deep Learning για φτωχά κινητά. Ίδιο αποτέλεσμα με λιγότερα μαθηματικά.
41;"Residual connections;";Αντιμετωπίζουν το degradation επιτρέποντας στο σήμα να προσπερνά layers.;Η παράκαμψη στην εθνική. Αν ένα layer είναι άχρηστο, το σήμα πάει από δίπλα.
42;"Learning identity mappings;";Κρίσιμο για να μπορεί ένα βαθύ δίκτυο να είναι τουλάχιστον όσο καλό όσο ένα ρηχό.;Αν δεν μπορείς να βελτιώσεις κάτι, τουλάχιστον μην το χαλάσεις.
43;"Inception modules;";Πολλαπλά μεγέθη kernels παράλληλα για εξαγωγή διαφορετικών κλιμάκων.;Ο μπουφές του CNN. Παίρνεις λίγο απ' όλα και ελπίζεις να δουλέψει.
44;"EfficientNet compound scaling;";Ισορροπημένη αύξηση βάθους, πλάτους και ανάλυσης.;Ο personal trainer του μοντέλου. Γυμνάζεις όλο το σώμα, όχι μόνο τα πόδια.
45;"FCN vs Sliding Window;";Τα FCNs επεξεργάζονται όλη την εικόνα ταυτόχρονα μέσω convolutions.;FCN: φωτογραφία. Sliding window: μεγεθυντικός φακός λέξη-λέξη.
46;"Μειονέκτημα FCNs;";Έλλειψη ακρίβειας στα bounding boxes λόγω του pooling.;"Ξέρει πού είσαι ""περίπου"". Ακρίβεια μεθυσμένου σε στόχο."
47;"Grid cells (YOLO);";Χωρίζουν την εικόνα σε περιοχές ευθύνης για τον εντοπισμό αντικειμένων.;Αν χωρίσεις την πίτσα σε 9 κομμάτια, σε κάθε κομμάτι χωράει μόνο μία ελιά.
48;"IoU vs Accuracy;";Το IoU μετράει την επικάλυψη των κουτιών, το accuracy είναι άχρηστο εδώ.;"Το ""σχεδόν μέσα"" δεν μετράει. Ή το βρήκες ή κοιτάς τον ουρανό."
49;"Multiple detections (NMS);";Το Non-Maximum Suppression κρατάει μόνο την πρόβλεψη με το υψηλότερο score.;Ο εκτελεστής που κρατάει τον ισχυρότερο και σβήνει τους υπόλοιπους 50.
50;"Object detection ως regression;";Πρόβλεψη συνεχών τιμών (x, y, w, h) αντί για κλάσεις.;"Δεν είναι ""ναι/όχι"", είναι ""πού ακριβώς"". Εφιάλτης για τον optimizer."
51;"Siamese Networks;";Μαθαίνουν ομοιότητα μεταξύ εισόδων χωρίς να ξέρουν την κλάση.;"Το ""βρες τις διαφορές"". Δεν ξέρω ποιος είναι ο Γιάννης, αλλά αυτός του μοιάζει."
52;"One-shot vs Few-shot;";One-shot: 1 δείγμα. Few-shot: λίγα δείγματα (π.χ. 5) ανά κλάση.;Αν το μοντέλο σου θέλει 1.000 δείγματα για να μάθει, δεν κάνει learning, κάνει παπαγαλία.
53;"Zero-shot Learning;";Αναγνώριση κλάσεων χωρίς κανένα οπτικό δείγμα, μόνο μέσω περιγραφής.;Φαντασία. Λες στο μοντέλο πώς μοιάζει ένας μονόκερος και περιμένεις να τον βρει.
54;"Meta-learning;";Learning to learn. Προσαρμογή σε νέα tasks με ελάχιστα δεδομένα.;Ο μάνατζερ των μοντέλων. Δεν λύνει το πρόβλημα, ξέρει πώς να το λύσει γρήγορα.
55;"Active Learning;";Το μοντέλο διαλέγει ποια δεδομένα πρέπει να γίνουν label από άνθρωπο.;Ρωτάς τον καθηγητή μόνο για τις σελίδες που δεν κατάλαβες.
56;"Self-supervised learning;";Μάθηση από τα ίδια τα δεδομένα χωρίς labels (π.χ. συμπλήρωση κενών).;Το μοντέλο παίζει με τον εαυτό του μέχρι να καταλάβει τον κόσμο.
57;"One-hot semantic failure;";Τα διανύσματα είναι ορθογώνια, άρα η ομοιότητα είναι πάντα μηδέν.;"Σαν να λες ότι το ""σκύλος"" και το ""γάτα"" δεν έχουν σχέση επειδή είναι σε άλλο ράφι."
58;"Distributional Hypothesis;";Οι λέξεις που εμφανίζονται στο ίδιο context έχουν παρόμοια σημασία.;Πες μου ποιον κάνεις παρέα να σου πω ποιος είσαι (εκδοχή NLP).
59;"Statistical vs Neural LM;";Τα neural γενικεύουν καλύτερα λόγω διανυσματικών αναπαραστάσεων.;Λογιστής με Excel vs Καλλιτέχνης που πιάνει το νόημα.
60;"Context length σε n-grams;";Το κόστος αυξάνεται εκθετικά, κάνοντας τη μνήμη αδύνατη.;Αν θες να θυμάσαι 10 λέξεις πίσω, θες μνήμη μεγαλύτερη από το σύμπαν.
61;"Embeddings ως byproduct;";Μαθαίνονται κατά την εκπαίδευση ενός language model.;Το σνακ που κλέβεις από την κουζίνα ενώ μαγειρεύεις το κύριο φαγητό.
62;"Low-dim embeddings;";Συμπύκνωση πληροφορίας σε λίγες διαστάσεις για ανάδειξη σχέσεων.;Αντί για χάρτη 50.000 δρόμων, έχεις GPS με 300 συντεταγμένες.
63;"King - Man + Woman;";Γεωμετρική απόδειξη σημασιολογικής αριθμητικής στο latent space.;Το μόνο που πείθει τους επενδυτές ότι το AI δεν είναι εντελώς ηλίθιο.
64;"Αποτυχία Embeddings;";Πολυσημία, σαρκασμός και σπάνιες λέξεις.;"Για το Word2Vec το ""κάθισμα της Βουλής"" είναι το ίδιο με το ""κάθισμα της καρέκλας""."
65;"CBoW vs Skip-gram;";CBoW: context->word. Skip-gram: word->context.;"CBoW: ""τι λείπει;"". Skip-gram: ""αν έχω αυτό, τι άλλο θα δω γύρω μου;""."
66;"Skip-gram και σπάνιες λέξεις;";Δημιουργεί περισσότερα pairs ανά λέξη, δίνοντας ευκαιρίες μάθησης.;Ακόμα και ο περιθωριακός του λεξιλογίου παίρνει το χρόνο του στο προσκήνιο.
67;"Hierarchical Softmax;";Μειώνει το κόστος από O(V) σε O(logV) μέσω δέντρων Huffman.;Ψάχνεις σε τηλεφωνικό κατάλογο αντί να ρωτάς κάθε άνθρωπο στη γη.
68;"GloVe co-occurrence;";Χρησιμοποιεί στατιστικά από όλο το κείμενο ταυτόχρονα.;Word2Vec για όσους αγαπούν τη στατιστική και τη μεγάλη εικόνα.
69;"OOV πρόβλημα;";Λέξεις εκτός training set δεν έχουν διάνυσμα (Unknown).;Αν δεν το έχει δει ποτέ, για το μοντέλο είναι απλά θόρυβος.
70;"FastText και Subwords;";Σπάει λέξεις σε n-grams χαρακτήρων για να καταλάβει ρίζες.;"Το μόνο που καταλαβαίνει ότι το ""έγραψα"" και το ""γράφω"" έχουν σχέση."
71;"BPE Tokenizer;";Δεν είναι embedding, είναι ο τρόπος που κόβουμε το κείμενο σε tokens.;Το BPE είναι το ψαλίδι, το embedding είναι το χρώμα. Μην τα μπερδεύεις.
72;"Subword Generalization;";Επιτρέπει τη διαχείριση άγνωστων λέξεων μέσω γνωστών κομματιών.;Lego: αν δεν έχεις το κάστρο, έχεις αρκετά τουβλάκια για να το φτιάξεις.
73;"BPE Trade-off;";Μικρό λεξιλόγιο σημαίνει μακρύτερες ακολουθίες (αργή επεξεργασία).;Διαλέγεις ανάμεσα σε πολλά μικρά τουβλάκια ή λίγα μεγάλα και έτοιμα.
74;"RNN Practical failure;";"Τα gradients ""σβήνουν"" μετά από λίγα βήματα (vanishing).";Θεωρητικά θυμάται τα πάντα, πρακτικά έχει Αλτσχάιμερ μετά από 5 λέξεις.
75;"RNN ως deep network;";Κάθε χρονικό βήμα είναι ένα layer. Πολλά βήματα = τεράστιο βάθος.;Κάθε λέξη είναι ένας πολλαπλασιασμός που στραγγίζει την πληροφορία.
76;"Gradient Clipping;";Κόβει τις ακραίες τιμές για να αποφευχθεί το explosion.;Ο κόφτης στο αμάξι. Σε σώζει από το τρακάρισμα, όχι από την έλλειψη βενζίνης.
77;"LSTM Gates;";Ελέγχουν τη ροή πληροφορίας για προστασία της μνήμης.;Οι θυρωροί της μνήμης. Αποφασίζουν τι κρατάς και τι πετάς.
78;"Cell State vs Hidden State;";Cell state = μακροπρόθεσμη μνήμη. Hidden state = τρέχουσα έξοδος.;Cell state: το αρχείο. Hidden state: αυτό που λες εκείνη τη στιγμή.
79;"Forget Gate;";Το πιο κρίσιμο gate για να διαγράφονται άχρηστες πληροφορίες.;"Το κουμπί ""Delete"". Αν δεν ξέρεις τι να ξεχάσεις, δεν μπορείς να μάθεις."
80;"Bidirectional LSTMs;";Βλέπουν το κείμενο και από τις δύο κατευθύνσεις.;Σαν να διαβάζεις μια πρόταση έχοντας ήδη δει το τέλος της. Cheat code.
81;"Bi-LSTMs και Online;";"Ακατάλληλα γιατί δεν υπάρχει ""μέλλον"" σε πραγματικό χρόνο.";Δεν μπορείς να δεις το μέλλον αν δεν έχει συμβεί ακόμα. Δεν είμαστε στην Πυθία.
82;"Autoregressive production;";Παραγωγή λέξης-λέξης όπου κάθε λέξη γίνεται input για την επόμενη.;Προσπαθείς να τελειώσεις μια πρόταση βασισμένος στην τελευταία σου λέξη.
83;"Softmax back to model;";"Δεν τροφοδοτούμε πιθανότητες γιατί το μοντέλο θέλει ""καθαρές"" εισόδους.";"Αν του δώσεις ""γατόσκυλο"" (30% γάτα, 20% σκύλο), θα μπερδευτεί τελείως."
84;"Beam Search vs Greedy;";Το Beam Search εξετάζει πολλά μονοπάτια ταυτόχρονα για καλύτερη συνολική πιθανότητα.;Το Greedy είναι ο ανυπόμονος, το Beam Search κοιτάζει όλο τον κατάλογο.
85;"Beam Width;";Μεγάλο width = ποιότητα αλλά αργό και βαρετό. Μικρό = ρίσκο και ποικιλία.;Όσο πιο πλατύ το beam, τόσο πιο συντηρητικό και κοινότοπο το κείμενο.
86;"Beam Search Global Optimum;";Δεν εγγυάται το βέλτιστο γιατί είναι heuristic και δεν ψάχνει τα πάντα.;Μια καλή προσέγγιση, αλλά όχι η απόλυτη αλήθεια.
87;"Conditional Probability;";Συνδέει τις λέξεις σε μια αλυσίδα πιθανοτήτων.;Αν η πρώτη λέξη είναι λάθος, όλη η πρόταση πάει στον κουβά.
88;"BERT για παραγωγή;";"Ακατάλληλος γιατί είναι bidirectional και ""κλέβει"" βλέποντας τη λέξη που πρέπει να βρει.";Ο BERT είναι για να διαβάζει, όχι για να γράφει. Διορθωτής κειμένων, όχι ποιητής.
89;"Bidirectional Attention;";Όλα τα tokens επικοινωνούν με όλα τα άλλα ταυτόχρονα.;Το απόλυτο κουτσομπολιό. Όλοι μιλάνε με όλους για να βγάλουν νόημα.
90;"Decoder-only Transformers;";Είναι autoregressive και χρησιμοποιούν masking για να μη βλέπουν το μέλλον.;Είναι τυφλοί προς τα δεξιά για να μάθουν να μαντεύουν σωστά.
91;"Tokenizer και pretrained;";"Η επιλογή tokenizer επηρεάζει άμεσα το τι ""καταλαβαίνει"" το μοντέλο.";"Αν ο tokenizer σου κόβει το Apple σε ""Ap"" και ""ple"", καλή τύχη."
92;"Αποσύνδεση Tokenizer-Model;";Οδηγεί σε πλήρη αποτυχία γιατί τα IDs δεν αντιστοιχούν στις σωστές λέξεις.;Σαν να αλλάζεις το πληκτρολόγιο κάποιου και το A να γράφει Ω.
93;"RNNs to Transformers;";Αλλαγή από ακολουθιακή σε παράλληλη επεξεργασία μέσω προσοχής.;Από το τρένο στην τηλεμεταφορά. Πιο γρήγορο, αλλά θέλει πολύ καύσιμο.
94;"AI workloads training vs inference;";Το training θέλει ισχύ, το inference θέλει ταχύτητα και οικονομία.;Training: μαραθώνιος. Inference: σπριντ. Μην παίρνεις Ferrari για το περίπτερο.
95;"GPU vs TPU;";Η GPU είναι ευέλικτη, η TPU είναι ειδικά σχεδιασμένη για tensors.;GPU: ελβετικός σουγιάς. TPU: ειδικό εργαλείο της Google.
96;"Memory Bandwidth;";"Συχνά πιο σημαντικό από τα FLOPs γιατί οι επεξεργαστές ""πεινάνε"" για data.";"Τι να τα κάνεις τα cores αν η μνήμη σου στέλνει δεδομένα με το πάσο της;"
97;"Storage Architecture;";Τα αργά δίκτυα αφήνουν τις GPUs idle, καταστρέφοντας το χρόνο εκπαίδευσης.;Ο πιο ακριβός τρόπος να μη φτιάξεις ποτέ το μοντέλο σου.
98;"Ψύξη Data Centers;";Η τεράστια θερμότητα των AI chips απαιτεί υδρόψυξη αντί για αέρα.;Τα AI DC είναι ηφαίστεια. Αν δεν βάλεις νερό, θα λιώσουν.
99;"Scaling up vs Scaling out;";Up: μεγαλύτερη μηχανή. Out: πολλές μηχανές σε δίκτυο.;Up: μεγαλύτερο σφυρί. Out: 100 άνθρωποι με μικρά σφυράκια.
100;"High-speed interconnect;";NVLink κτλ επιτρέπουν στις GPUs να μιλάνε γρήγορα μεταξύ τους.;Η εθνική οδός των δεδομένων. Χωρίς αυτή, έχεις μποτιλιάρισμα.
101;"Managed Cloud Services;";Επιταχύνουν τον πειραματισμό αλλά κοστίζουν ακριβά μακροπρόθεσμα.;Το Colab είναι για παιχνίδι, το On-prem για σοβαρή δουλειά.
102;"Carbon footprint;";Το AI καταναλώνει τεράστια ενέργεια, κάνοντας την απόδοση ηθική ανάγκη.;Κάθε φορά που πατάς fit(), ένας παγετώνας λιώνει.
103;"Cloud vs On-premise;";Επιλογή μεταξύ ευκολίας και κόστους/ασφάλειας.;Cloud: νοίκι στον Bezos. On-prem: δικό σου σπίτι, αλλά καθαρίζεις εσύ.
104;"Γιατροί vs AI;";Το AI βρίσκει patterns, ο γιατρός έχει context και ευθύνη.;Θέλεις τον γιατρό για να έχεις κάποιον να μηνύσεις αν πάει κάτι στραβά.
105;"AI και Ιδιωτικότητα;";Υπάρχουν τεχνικές προστασίας, αλλά η πλήρης εγγύηση είναι δύσκολη.;"Το ""μην ανησυχείτε"" του Big Brother."
106;"Ανθρώπινη επίβλεψη;";Απαραίτητη γιατί το AI δεν έχει κοινή λογική και κάνει επικίνδυνα λάθη.;Χωρίς άνθρωπο, το AI είναι απλώς ένας γρήγορος τρόπος να κάνεις μαζικές βλακείες.
